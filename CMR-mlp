lass MLP(object):
    

    def __init__(self, rng, input, n_in, n_hidden, n_out):
        """初始化多层感知器的参数
       -----参数解释-----
        :type rng: numpy.random.RandomState
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.TensorType
        :param input: symbolic variable that describes the input of the
        architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
        which the datapoints lie

        :type n_hidden: int
        :param n_hidden: number of hidden units 隐藏单元数

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
        which the labels lie  输出单元数

        """

        
        self.hiddenLayer = HiddenLayer(rng=rng, input=input,
                                 n_in=n_in, n_out=n_hidden,
                                 activation=T.tanh, name_prefix='hid_')

        # 逻辑回归层以隐藏单位作为输入隐藏层
        self.logRegressionLayer = LogisticRegression(
                                    input=self.hiddenLayer.output,
                                    n_in=n_hidden,
                                    n_out=n_out, name_prefix='log_')

        # MLP的负对数似然性由负的对数似然性给出对数似然输出的模型，计算在逻辑回归层
        self.negative_log_likelihood = self.logRegressionLayer.negative_log_likelihood

        ## 模型的参数是它所在的两层的参数合成
        self.params = self.hiddenLayer.params + self.logRegressionLayer.params


def test_mlp():
    """
    演示一个多层随机梯度下降优化感知器
    ##参数解释##
    :type learning_rate: float
    :param learning_rate: learning rate used (factor for the stochastic
    gradient
    :type n_epochs: int
    :param n_epochs: maximal number of epochs to run the optimizer
    :type dataset: string
    
   """
    datasets = gen_data()

    train_set_x, train_set_y = datasets[0]
    valid_set_x, valid_set_y = datasets[1]
    test_set_x , test_set_y  = datasets[2]

    batch_size = 100     # 批量的大小

    # 计算用于训练、验证和测试的小批量的数量,
    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size
    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size
    n_test_batches  = test_set_x.get_value(borrow=True).shape[0]  / batch_size

    ######################
    # 建立实际模型 #
    ######################
   
    # 数据分配符号变量
    index = T.lscalar()     # [mini]batch的索引
    x     = T.matrix('x')   # 数据以光栅化图像的形式呈现
    y     = T.ivector('y')  # 标签以整型[int]标签的一维向量表示
                          

    rng = np.random.RandomState(1234)

    # 构建MLP类
    classifier = MLP( rng=rng, input=x, n_in=28*28, n_hidden=500, n_out=10)  # MLP 输入是28*28，隐藏神经元为500，输出为10

    # 在训练过程中最小化的代价是的负对数可能性模型
    
    # 取每一小批的成本的平均值.
    cost = classifier.negative_log_likelihood(y).mean()

    # 计算代价关于的梯度 (stored in params)
    # 产生的梯度将存储在一个gparams列表中
    gparams = []
    for param in classifier.params:
        gparam  = T.grad(cost, param)
        gparams.append(gparam)

    # 一些需要的优化被标记为“fast_run”
         
    mode = theano.compile.get_default_mode().including('fast_run')
 
    updates2 = OrderedDict()
        
    updates2[classifier.hiddenLayer.params[0]] = T.grad(cost, classifier.hiddenLayer.params[0])
    train_model = theano.function( inputs=[index],
            updates=updates2,
            givens={
                x: train_set_x[index*batch_size:(index+1)*batch_size],
                y: train_set_y[index*batch_size:(index+1)*batch_size]},
            mode=mode)
            
    # 打印 '模型 1'
    #theano.printing.debugprint(train_model, print_type=True)
    assert any([isinstance(i.op, T.nnet.CrossentropySoftmax1HotWithBiasDx) for i in train_model.maker.fgraph.toposort()])

    train_model = theano.function( inputs=[index],
            updates=updates2,
            mode=mode.excluding('ShapeOpt'),
            givens={
                x: train_set_x[index*batch_size:(index+1)*batch_size],
                y: train_set_y[index*batch_size:(index+1)*batch_size]})


    # 打印 '模型 2'
    #theano.printing.debugprint(train_model, print_type=True)
    assert any([isinstance(i.op, T.nnet.CrossentropySoftmax1HotWithBiasDx) for i in train_model.maker.fgraph.toposort()])
