lass MLP(object):
    """Multi-Layer Perceptron Class

    A multilayer perceptron is a feedforward artificial neural network model
    that has one layer or more of hidden units and nonlinear activations.
    Intermidiate layers usually have as activation function thanh or the
    sigmoid function (defined here by a ``SigmoidalLayer`` class)  while the
    top layer is a softamx layer (defined here by a ``LogisticRegression``
    class).
    """

    def __init__(self, rng, input, n_in, n_hidden, n_out):
        """初始化多层感知器的参数
       -----参数解释-----
        :type rng: numpy.random.RandomState
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.TensorType
        :param input: symbolic variable that describes the input of the
        architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
        which the datapoints lie

        :type n_hidden: int
        :param n_hidden: number of hidden units 隐藏单元数

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
        which the labels lie  输出单元数

        """

        # Since we are dealing with a one hidden layer MLP, this will
        # translate into a TanhLayer connected to the LogisticRegression
        # layer; this can be replaced by a SigmoidalLayer, or a layer
        # implementing any other nonlinearity
        self.hiddenLayer = HiddenLayer(rng=rng, input=input,
                                 n_in=n_in, n_out=n_hidden,
                                 activation=T.tanh, name_prefix='hid_')

        # The logistic regression layer gets as input the hidden units
        # of the hidden layer
        self.logRegressionLayer = LogisticRegression(
                                    input=self.hiddenLayer.output,
                                    n_in=n_hidden,
                                    n_out=n_out, name_prefix='log_')

        # MLP的负对数似然性由负的对数似然性给出对数似然输出的模型，计算在逻辑回归层
        self.negative_log_likelihood = self.logRegressionLayer.negative_log_likelihood

        ## 模型的参数是它所在的两层的参数合成
        self.params = self.hiddenLayer.params + self.logRegressionLayer.params


def test_mlp():
    """
       演示一个多层随机梯度下降优化感知器

    :type learning_rate: float
    :param learning_rate: learning rate used (factor for the stochastic
    gradient

    :type n_epochs: int
    :param n_epochs: maximal number of epochs to run the optimizer

    :type dataset: string
    :param dataset: the path of the MNIST dataset file from
                         http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz
   """
    datasets = gen_data()

    train_set_x, train_set_y = datasets[0]
    valid_set_x, valid_set_y = datasets[1]
    test_set_x , test_set_y  = datasets[2]

    batch_size = 100     # 批量的大小

    # 计算用于培训的小批量的数量, 验证和测试
    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size
    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size
    n_test_batches  = test_set_x.get_value(borrow=True).shape[0]  / batch_size

    ######################
    # 建立实际模型 #
    ######################
   
    # 数据分配符号变量
    index = T.lscalar()     # [mini]batch的索引
    x     = T.matrix('x')   # 数据以光栅化图像的形式呈现
    y     = T.ivector('y')  # 标签以整型[int]标签的一维向量表示
                          

    rng = np.random.RandomState(1234)

    # 构建MLP类
    classifier = MLP( rng=rng, input=x, n_in=28*28, n_hidden=500, n_out=10)  # MLP 输入是28*28，隐藏神经元为500，输出为10

    # 在训练过程中最小化的代价是的负对数可能性模型
    
    # 取每一小批的成本的平均值.
    cost = classifier.negative_log_likelihood(y).mean()

    # 计算代价关于的梯度 (stored in params)
    # 产生的梯度将存储在一个gparams列表中
    gparams = []
    for param in classifier.params:
        gparam  = T.grad(cost, param)
        gparams.append(gparam)

    # 一些需要的优化被标记为“fast_run”
    # TODO: refine that and include only those
         
    mode = theano.compile.get_default_mode().including('fast_run')
 
    updates2 = OrderedDict()
        
    updates2[classifier.hiddenLayer.params[0]] = T.grad(cost, classifier.hiddenLayer.params[0])
    train_model = theano.function( inputs=[index],
            updates=updates2,
            givens={
                x: train_set_x[index*batch_size:(index+1)*batch_size],
                y: train_set_y[index*batch_size:(index+1)*batch_size]},
            mode=mode)
            
    # 打印 '模型 1'
    #theano.printing.debugprint(train_model, print_type=True)
    assert any([isinstance(i.op, T.nnet.CrossentropySoftmax1HotWithBiasDx) for i in train_model.maker.fgraph.toposort()])

    train_model = theano.function( inputs=[index],
            updates=updates2,
            mode=mode.excluding('ShapeOpt'),
            givens={
                x: train_set_x[index*batch_size:(index+1)*batch_size],
                y: train_set_y[index*batch_size:(index+1)*batch_size]})


    # 打印 '模型 2'
    #theano.printing.debugprint(train_model, print_type=True)
    assert any([isinstance(i.op, T.nnet.CrossentropySoftmax1HotWithBiasDx) for i in train_model.maker.fgraph.toposort()])
